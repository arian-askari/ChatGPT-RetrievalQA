{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "collapsed_sections": [
        "K0Viw02D_piA",
        "76iDBJg_Ot9m",
        "vFjBKFvtnaNG",
        "Ddxuwiacjc4H",
        "IZVtHoHL8qfA",
        "pt8Sgna4zvps"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "K0Viw02D_piA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jsonlines\n",
        "import json\n",
        "import tqdm\n",
        "import jsonlines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "COLAB_RUN = True\n",
        "if COLAB_RUN:\n",
        "  base_path = \"./gdrive/MyDrive/ChatGPT-RetrievalQA-private/\"\n",
        "  prompts_base_path = \"./gdrive/MyDrive/ChatGPT-RetrievalQA-private/prompts/\"\n",
        "  dataset_path = \"./gdrive/MyDrive/ChatGPT-RetrievalQA/\"\n",
        "else:\n",
        "  base_path = \"./\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X14r2c-y_rkq",
        "outputId": "00aede6d-08b9-498b-d5ef-dcb8102b12bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jsonlines\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonlines) (22.2.0)\n",
            "Installing collected packages: jsonlines\n",
            "Successfully installed jsonlines-3.1.0\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n"
      ],
      "metadata": {
        "id": "76iDBJg_Ot9m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### read collections"
      ],
      "metadata": {
        "id": "vFjBKFvtnaNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_collection(f_path):\n",
        "  corpus = {}\n",
        "  with open(f_path, \"r\") as fp:\n",
        "    for line in tqdm.tqdm(fp, desc=\"reading {}\".format(f_path)):\n",
        "      did, dtext = line.strip().split(\"\\t\")\n",
        "      corpus[did] = dtext\n",
        "  return corpus"
      ],
      "metadata": {
        "id": "clfEPg9TnbVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### normalize content function\n"
      ],
      "metadata": {
        "id": "Ddxuwiacjc4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_content(content):\n",
        "  return content.replace('\\r', '').replace('\\n', '').replace('\\t', ' ')"
      ],
      "metadata": {
        "id": "9jlzmv10jhGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### write to file function"
      ],
      "metadata": {
        "id": "IZVtHoHL8qfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_to_file(output_path, content):\n",
        "  f_w = open(output_path, \"w+\")\n",
        "  f_w.write(\"\".join(content))\n",
        "  f_w.close()"
      ],
      "metadata": {
        "id": "RoTvecrm8hRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model"
      ],
      "metadata": {
        "id": "pt8Sgna4zvps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "m-JK4f0824o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "os.environ[\"HF_ENDPOINT\"] = \"https://huggingface.co\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom\")"
      ],
      "metadata": {
        "id": "P_iSJS1dzxJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt template class"
      ],
      "metadata": {
        "id": "Whvnj46dx9xD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptTemplate:\n",
        "  def __init__(self, model, tokenizer, template= \"{}\", prefix= \"\", max_new_tokens = 200, edit_output = False):\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "    self.template = template\n",
        "    self.prefix = prefix\n",
        "    self.max_new_tokens = max_new_tokens\n",
        "    self.edit_output = edit_output\n",
        "  # loading the template from the file. The template can be passes as string too.\n",
        "  def load_template(self, tempalte_path):\n",
        "    try: \n",
        "      self.template = open(tempalte_path, \"r\").read()\n",
        "    except Exception as e:\n",
        "      pass\n",
        "    try:\n",
        "      self.template = open(tempalte_path, \"r\").read()\n",
        "    except Exception as e:\n",
        "      print(\"error: \", e)\n",
        "  # main function to generate the text. \n",
        "  def generate_text(self):\n",
        "      return \"Implement this function on children classes :)\"\n",
        "  # Checking some rules before generating the text\n",
        "  def check_rules(self, input_str):\n",
        "    return True\n",
        "  # pick the infromation from the ouput of the model. It is especially needed for autoregressive models. For seq2seq models you will only get the text and the select_information do not need to be overrided\n",
        "  def select_information(self, output_str):\n",
        "    return output_str\n",
        "  # generate text basic function with a list of prompts arguments\n",
        "  def generate_text_basic(self, prompt_arg):\n",
        "    if type(prompt_arg) == str:\n",
        "      prompt_arg = [prompt_arg]\n",
        "    template = self.prefix + self.template.format(*prompt_arg)\n",
        "    tokens = tokenizer(template, return_tensors=\"pt\").to(0)\n",
        "    outputs = model.generate(input_ids=tokens[\"input_ids\"], attention_mask=tokens[\"attention_mask\"], max_new_tokens = self.max_new_tokens, eos_token_id= tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id)\n",
        "    str_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return str_output"
      ],
      "metadata": {
        "id": "q1Qk3HXWyR8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📶 Explanding query\n"
      ],
      "metadata": {
        "id": "lke9BxQfueAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class"
      ],
      "metadata": {
        "id": "9NkxJn-G09vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpandingQuery(PromptTemplate):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(ExpandingQuery, self).__init__(*args, **kwargs)\n",
        "  # query with more than 10 words won't be expanded\n",
        "  def check_rules(self, input_str):\n",
        "    long_query_words = 10\n",
        "    long_query_chars = 10 * 4.7\n",
        "    if len(input_str)> long_query_chars: \n",
        "      return False # query with more than 10 words is lengthy enough and we do not expand it :) we consider a query with more than 10 words as a long query inspired by: https://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf, section 2, paragraph 2.\n",
        "  def select_information(self, str_output):\n",
        "    select = str_output.split(\"Example 4:\\n\")[1].split(\"Query Expanded:\")[1].split(\"\\n\")[0]\n",
        "    return select\n",
        "  def generate_text(self, query):\n",
        "    status = False\n",
        "    if self.check_rules(query) == False:\n",
        "      return (query, status) # if nothing happens because of the rules, then return false to show we return original content.\n",
        "    str_output = super(ExpandingQuery, self).generate_text_basic(query)\n",
        "    return (self.select_information(str_output).strip(), True) # True is status! it means we really expanded the query!"
      ],
      "metadata": {
        "id": "32BVy3ibx7Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo "
      ],
      "metadata": {
        "id": "KfSBd0Ez0-sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_expander = ExpandingQuery(model, tokenizer)\n",
        "query_expander.template = \"\"\"Expand the query but do not change the main question.\n",
        "\n",
        "Example 1:\n",
        "Query: Is a little caffeine ok during pregnancy?\n",
        "Query Expanded: What is the recommended amount of caffeine intake during pregnancy, and are there any potential risks associated with consuming small amounts of caffeine while pregnant?\n",
        "\n",
        "Example 2:\n",
        "Query: What fruit is native to Australia?\n",
        "Query Expanded: Which fruit is exclusive to Australia and provide some additional details about it?\n",
        "\n",
        "Example 3:\n",
        "Query: How large is the canadian military?\n",
        "Query Expanded: What is the size of the canadian military ahd what is the number of active personnel and reserve members?\n",
        "\n",
        "Example 4:\n",
        "Query: {}\n",
        "Query Expanded:\"\"\"\n",
        "generated_text, status = query_expander.generate_text(\"what are gases used in neon signs?\")\n",
        "print(\"generated_text: \", generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBkHUcV40rI1",
        "outputId": "7666bc8d-3822-4c15-9505-ab29a6fc066f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated_text:  Is there a list of gases that are used for neon sign identification?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  🖋 TBD "
      ],
      "metadata": {
        "id": "tZRel--W8cRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  🔨 TBD"
      ],
      "metadata": {
        "id": "e5U4lXS8Dveb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ❓ TBD\n"
      ],
      "metadata": {
        "id": "MLukvONTsVnO"
      }
    }
  ]
}